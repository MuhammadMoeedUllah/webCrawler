	
service: web-crawler
 
plugins:
  - serverless-python-requirements
  - serverless-wsgi

custom:
  tableName: 'scrapedWebPage-${self:provider.stage}'
  wsgi:
    app: src/app.app
    packRequirements: false
  pythonRequirements:
    dockerizePip: non-linux
 
provider:
  name: aws
  runtime: python3.6
  stage: dev
  region: us-east-1
  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:Query
        - dynamodb:Scan
        - dynamodb:GetItem
        - dynamodb:PutItem
        - dynamodb:UpdateItem
        - dynamodb:DeleteItem
      Resource:
        - { "Fn::GetAtt": ["ScrapedWebPageTable", "Arn" ] }
    - Effect: Allow
      Action:
        - sqs:*
      Resource:
        Fn::GetAtt: [ apiRequestsQueue, Arn ]   
  environment:
    REQUESTS_TABLE: ${self:custom.tableName}
    SQS_QUEUE_URL: { Ref: apiRequestsQueue }
    
functions:
  app:
    handler: wsgi_handler.handler
    events:
      - http:
          path: /
          method: ANY
      - http:
          path: /{proxy+}
          method: ANY
  worker:
    handler: src/sqs/handler.handler
    events:
      - sqs:
          arn:
            Fn::GetAtt:
              - apiRequestsQueue
              - Arn
          batchSize: 10
          maximumBatchingWindow: 1
          functionResponseType: ReportBatchItemFailures  
 
resources:
  Resources:
    apiRequestsQueue:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: "apiRequestsQueue"
 
    ScrapedWebPageTable:
      Type: 'AWS::DynamoDB::Table'
      Properties:
        AttributeDefinitions:
          -
            AttributeName: requestId
            AttributeType: S
        KeySchema:
          -
            AttributeName: requestId
            KeyType: HASH
        ProvisionedThroughput:
          ReadCapacityUnits: 1
          WriteCapacityUnits: 1
        TableName: ${self:custom.tableName}
